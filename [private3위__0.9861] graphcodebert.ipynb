{"cells":[{"cell_type":"markdown","metadata":{"id":"0_axEaWxCYRw"},"source":["# 라이브러리 및 seed고정"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dp0FC0O0-FbJ","outputId":"2dd491ef-6d58-486e-de9a-d6fff00eb875"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["#from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMUKMgbL2T0u"},"outputs":[],"source":["!pip install transformers torch pandas tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nGzJzNdP-LUs"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import random\n","from itertools import combinations, product\n","import re\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","import torch\n","torch.set_float32_matmul_precision('high')\n","from torch.utils.data import Dataset, DataLoader\n","import transformers\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n","from tqdm import tqdm\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","#from google.colab import files"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wyNlPG1plJQd","outputId":"9b810a3e-2541-47ab-af8f-87147a1a5eae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Seed set as 42\n"]}],"source":["# Seed 고정 함수\n","def seed_everything(seed: int = 42, contain_cuda: bool = False):\n","  os.environ['PYTHONHASHSEED'] = str(seed)\n","  random.seed(seed)\n","  np.random.seed(seed)\n","\n","  torch.backends.cudnn.deterministic = True\n","  torch.backends.cudnn.benchmark = False\n","\n","  torch.manual_seed(seed)\n","  torch.cuda.manual_seed(seed)\n","  torch.cuda.manual_seed_all(seed)\n","  print(f\"Seed set as {seed}\")\n","\n","seed = 42\n","seed_everything(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4R86ps4-lL3h"},"outputs":[],"source":["# CUDA 사용 가능 여부 확인 및 GPU 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"markdown","metadata":{"id":"XMOUB4_pnPby"},"source":["# 데이터 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CiR6kyogrYse"},"outputs":[],"source":["def preprocess_and_remove_extras(filepath):\n","    with open(filepath, 'r', encoding='utf-8') as file:\n","        content = file.read()\n","\n","    content = re.sub(re.compile(\"/\\*.*?\\*/\", re.DOTALL), \"\", content) # 멀티 라인 주석 제거\n","    content = re.sub(re.compile(\"//.*?\\n\"), \"\", content) # 싱글 라인 주석 제거\n","    content = re.sub(re.compile(\"#include <.*?>\\n\"), \"\", content) # angle brackets를 사용하는 include 제거\n","    content = re.sub(re.compile(\"#include \\\".*?\\\"\\n\"), \"\", content) # double quotes를 사용하는 include 제거\n","    content = re.sub(re.compile(\"#define .*?\\n\"), \"\", content) # 매크로 정의 제거\n","    content = re.sub(re.compile(\"[\\t ]+\"), \" \", content) # 공백 및 탭 정리\n","    content = re.sub(re.compile(\"\\n\\s*\\n\"), \"\\n\", content)# 여러 줄바꿈을 하나로\n","\n","    # 공백이 아닌 줄만 선택하여 리스트로 만든 후, 문자열로 결합\n","    processed_script = '\\n'.join([line.strip() for line in content.splitlines() if line.strip()])\n","\n","    return processed_script\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"77v85WAmd4rz"},"outputs":[],"source":["code_folder = \"./data/train_code\"\n","problem_folders = os.listdir(code_folder)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lcyrGsS-eJeN","outputId":"930fa526-ff26-4346-b065-25e4a09b054b"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [1:50:37<00:00, 13.28s/it]\n"]}],"source":["preprocess_scripts = []\n","problem_nums = []\n","\n","# 500개 Sample code에 대한 전처리\n","for problem_folder in tqdm(problem_folders):\n","    scripts = os.listdir(os.path.join(code_folder, problem_folder)) # code/problem000/.cpp 파일\n","    problem_num = problem_folder # 문제 번호 폴더명\n","    for script in scripts:\n","        # .ipynb_checkpoints 폴더 무시\n","        if script == \".ipynb_checkpoints\":\n","            continue\n","        script_file = os.path.join(code_folder,problem_folder,script)\n","        if os.path.isfile(script_file):  # 경로가 실제 파일인지 확인\n","            preprocessed_script =  preprocess_and_remove_extras(script_file)\n","            if preprocessed_script:  # 전처리된 스크립트가 비어 있지 않다면\n","                preprocess_scripts.append(preprocessed_script)\n","                problem_nums.append(problem_folder)  # 문제 번호 추가\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mVfAybvIeaaD"},"outputs":[],"source":["df = pd.DataFrame(data= {'code':preprocess_scripts, 'problem_num':problem_nums})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4cOcE7EDlbn4"},"outputs":[],"source":["# train과 validation data set 분리\n","train_df, valid_df, train_label, valid_label = train_test_split(\n","        df,\n","        df['problem_num'],\n","        random_state=42,\n","        test_size=0.1,\n","        stratify=df['problem_num']\n","    )\n","\n","train_df = train_df.reset_index(drop=True) # Reindexing\n","valid_df = valid_df.reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{"id":"o-9aGyX7m6j2"},"source":["# train data - 150만쌍"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hvq5JY3Nlsf2"},"outputs":[],"source":["codes = train_df['code'].to_list() # code 컬럼을 list로 변환 - codes는 code가 쭉 나열된 형태임\n","problems = train_df['problem_num'].unique().tolist() # 문제 번호를 중복을 제외하고 list로 변환\n","problems.sort()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G18Le2sNluNF"},"outputs":[],"source":["train_positive_pairs = []\n","train_negative_pairs = []"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L_t8N6Nvlxdl","outputId":"4e9f9b03-6e1a-4549-abb1-040e92294021"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [04:04<00:00,  2.04it/s]\n"]}],"source":["for problem in tqdm(problems):\n","    # 각각의 문제에 대한 code를 골라 정답 코드로 저장, 아닌 문제는 other_codes로 저장\n","    # 이때 train_df에는 problem_num이 정렬된 상태가 아니기 때문에 index가 다를 수 있음\n","    solution_codes = train_df[train_df['problem_num'] == problem]['code'].to_list()\n","    other_codes = train_df[train_df['problem_num'] != problem]['code'].to_list()\n","\n","    # positive_pairs 1500개 (총 500 * 1500 = 750,000개) 추출\n","    # negative_pairs 1500개 (총 500 * 1500 = 750,000개) 추출\n","    positive_pairs = list(combinations(solution_codes,2))\n","    random.shuffle(positive_pairs)\n","    positive_pairs = positive_pairs[:1500]\n","    random.shuffle(other_codes)\n","    other_codes = other_codes[:1500]\n","\n","    negative_pairs = []\n","    for pos_codes, others in zip(positive_pairs, other_codes):\n","        negative_pairs.append((pos_codes[0], others))\n","\n","    train_positive_pairs.extend(positive_pairs)\n","    train_negative_pairs.extend(negative_pairs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f09wHbZOl68R"},"outputs":[],"source":["# total_positive_pairs와 negative_pairs의 정답 코드를 묶어 code1로 지정\n","# total_positive_pairs와 negative_pairs의 비교 대상 코드를 묶어 code2로 지정\n","# 해당 코드에 맞는 label 설정\n","code1 = [code[0] for code in train_positive_pairs] + [code[0] for code in train_negative_pairs]\n","code2 = [code[1] for code in train_positive_pairs] + [code[1] for code in train_negative_pairs]\n","label = [1]*len(train_positive_pairs) + [0]*len(train_negative_pairs)\n","\n","# DataFrame으로 선언\n","create_train_data = pd.DataFrame(data={'code1':code1, 'code2':code2, 'similar':label})\n","create_train_df = create_train_data.sample(frac=1).reset_index(drop=True) # frac: 추출할 표본 비율\n"]},{"cell_type":"markdown","metadata":{"id":"xkYNRiroi65r"},"source":["# train data - 200만쌍"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DN3VJclHi65s"},"outputs":[],"source":["codes = train_df['code'].to_list() # code 컬럼을 list로 변환 - codes는 code가 쭉 나열된 형태임\n","problems = train_df['problem_num'].unique().tolist() # 문제 번호를 중복을 제외하고 list로 변환\n","problems.sort()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z2Xik4KYi65s"},"outputs":[],"source":["train_positive_pairs = []\n","train_negative_pairs = []"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cZkSzSpOi65t","outputId":"7d4998bf-f527-4f7d-d342-91eac2017472"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [04:05<00:00,  2.04it/s]\n"]}],"source":["for problem in tqdm(problems):\n","    # 각각의 문제에 대한 code를 골라 정답 코드로 저장, 아닌 문제는 other_codes로 저장\n","    # 이때 train_df에는 problem_num이 정렬된 상태가 아니기 때문에 index가 다를 수 있음\n","    solution_codes = train_df[train_df['problem_num'] == problem]['code'].to_list()\n","    other_codes = train_df[train_df['problem_num'] != problem]['code'].to_list()\n","\n","    # positive_pairs 2000개 (총 500 * 2000 = 1000,000개) 추출\n","    # negative_pairs 2000개 (총 500 * 2000 = 1000,000개) 추출\n","    positive_pairs = list(combinations(solution_codes,2))\n","    random.shuffle(positive_pairs)\n","    positive_pairs = positive_pairs[:2000]\n","    random.shuffle(other_codes)\n","    other_codes = other_codes[:2000]\n","\n","    negative_pairs = []\n","    for pos_codes, others in zip(positive_pairs, other_codes):\n","        negative_pairs.append((pos_codes[0], others))\n","\n","    train_positive_pairs.extend(positive_pairs)\n","    train_negative_pairs.extend(negative_pairs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JkGApdf5i65u"},"outputs":[],"source":["# total_positive_pairs와 negative_pairs의 정답 코드를 묶어 code1로 지정\n","# total_positive_pairs와 negative_pairs의 비교 대상 코드를 묶어 code2로 지정\n","# 해당 코드에 맞는 label 설정\n","code1 = [code[0] for code in train_positive_pairs] + [code[0] for code in train_negative_pairs]\n","code2 = [code[1] for code in train_positive_pairs] + [code[1] for code in train_negative_pairs]\n","label = [1]*len(train_positive_pairs) + [0]*len(train_negative_pairs)\n","\n","# DataFrame으로 선언\n","create2_train_data = pd.DataFrame(data={'code1':code1, 'code2':code2, 'similar':label})\n","create2_train_df = create2_train_data.sample(frac=1).reset_index(drop=True) # frac: 추출할 표본 비율\n"]},{"cell_type":"markdown","metadata":{"id":"FIeWfNwHmz1N"},"source":["# valid data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Chl9brF2l92w"},"outputs":[],"source":["codes = valid_df['code'].to_list() # code 컬럼을 list로 변환 - codes는 code가 쭉 나열된 형태임\n","problems = valid_df['problem_num'].unique().tolist() # 문제 번호를 중복을 제외하고 list로 변환\n","problems.sort()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJwteYfpl-Vw"},"outputs":[],"source":["valid_positive_pairs = []\n","valid_negative_pairs = []"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IErODHjNmCg_","outputId":"cf0c8a4c-17cd-40e6-eaca-b4550a39aac7"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [00:17<00:00, 29.07it/s]\n"]}],"source":["for problem in tqdm(problems):\n","    # 각각의 문제에 대한 code를 골라 정답 코드로 저장, 아닌 문제는 other_codes로 저장\n","    # 이때 train_df에는 problem_num이 정렬된 상태가 아니기 때문에 index가 다를 수 있음\n","    solution_codes = valid_df[valid_df['problem_num'] == problem]['code'].to_list()\n","    other_codes = valid_df[valid_df['problem_num'] != problem]['code'].to_list()\n","\n","    # positive_pairs 200개 (총 500 * 200 = 100,000개) 추출\n","    # negative_pairs 200개 (총 500 * 200 = 100,000개) 추출\n","    positive_pairs = list(combinations(solution_codes,2))\n","    random.shuffle(positive_pairs)\n","    positive_pairs = positive_pairs[:200]\n","    random.shuffle(other_codes)\n","    other_codes = other_codes[:200]\n","\n","    negative_pairs = []\n","    for pos_codes, others in zip(positive_pairs, other_codes):\n","        negative_pairs.append((pos_codes[0], others))\n","\n","    valid_positive_pairs.extend(positive_pairs)\n","    valid_negative_pairs.extend(negative_pairs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hIE1RmsDmIbL"},"outputs":[],"source":["# total_positive_pairs와 negative_pairs의 정답 코드를 묶어 code1로 지정\n","# total_positive_pairs와 negative_pairs의 비교 대상 코드를 묶어 code2로 지정\n","# 해당 코드에 맞는 label 설정\n","code1 = [code[0] for code in valid_positive_pairs] + [code[0] for code in valid_negative_pairs]\n","code2 = [code[1] for code in valid_positive_pairs] + [code[1] for code in valid_negative_pairs]\n","label = [1]*len(valid_positive_pairs) + [0]*len(valid_negative_pairs)\n","\n","# DataFrame으로 선언\n","valid_data = pd.DataFrame(data={'code1':code1, 'code2':code2, 'similar':label})\n","val_df = valid_data.sample(frac=1).reset_index(drop=True) # frac: 추출할 표본 비율\n"]},{"cell_type":"markdown","metadata":{"id":"GBeyHGD1j_xC"},"source":["# train data 합치기, 데이터 셋 클래스 정의\n","\n","train1_df: 새로 만든 150만개의 create_train_df + sample train data = 총 152만개\n","\n","train2_df: 새로 만든 200만개의 create_train_df + sample train data = 총 202만개"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"enrZ0iHcNdQi"},"outputs":[],"source":["sample_data_path = './data/sample_train.csv' # 예시로 주어진 샘플 데이터도 학습데이터에 포함\n","\n","sample_df = pd.read_csv(sample_data_path)\n","train1_df = pd.concat([create_train_df, sample_df], ignore_index=True) # 총 152만개의 train데이터 생성\n","train2_df = pd.concat([create2_train_df, sample_df], ignore_index=True) # 총 202만개의 train데이터 생성\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pscIYGOtN6Qf"},"outputs":[],"source":["#  전처리 적용\n","#  이미 create_train_data 만들 때 적용했지만, test.csv와, sample_train.csv에도 같은 전처리를 하기 위해 다시 정의\n","def remove_extras(code):\n","    code = re.sub(re.compile(\"/\\*.*?\\*/\", re.DOTALL), \"\", code) # 멀티 라인 주석 제거\n","    code = re.sub(re.compile(\"//.*?\\n\"), \"\", code) # 싱글 라인 주석 제거\n","    code = re.sub(re.compile(\"#include <.*?>\\n\"), \"\", code)  # angle brackets를 사용하는 include 제거\n","    code = re.sub(re.compile(\"#include \\\".*?\\\"\\n\"), \"\", code)  # double quotes를 사용하는 include 제거\n","    code = re.sub(re.compile(\"#define .*?\\n\"), \"\", code)  # 매크로 정의 제거\n","    code = re.sub(re.compile(\"[\\t ]+\"), \" \", code)  # 탭과 여러 공백을 하나의 공백으로\n","    code = re.sub(re.compile(\"\\n\\s*\\n\"), \"\\n\", code)  # 여러 줄바꿈을 하나로\n","\n","    return code.strip()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IJ3QF_vTWrYN"},"outputs":[],"source":["# 데이터셋 클래스 정의\n","# 여기서 전처리 함수는 remove_extras\n","class CodePairDataset(Dataset):\n","    def __init__(self, tokenizer, data, max_length=512, include_labels=True):\n","        self.tokenizer = tokenizer\n","        self.data = data\n","        self.max_length = max_length\n","        self.include_labels = include_labels\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        record = self.data.iloc[idx]\n","        code1 = remove_extras(record['code1'])\n","        code2 = remove_extras(record['code2'])\n","\n","        inputs = self.tokenizer(\n","            code1, code2,\n","            padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n","        )\n","        inputs = {key: val.squeeze() for key, val in inputs.items()}\n","        if self.include_labels:\n","            inputs['labels'] = torch.tensor(record['similar'], dtype=torch.long)\n","        return inputs\n"]},{"cell_type":"markdown","metadata":{"id":"yZpH35y3mHRb"},"source":["# 첫번째 모델\n","\n","152만쌍 train, epoch 4, lr = 2e-5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGNQtDee2hnI"},"outputs":[],"source":["# GraphCodeBERT 모델 및 토크나이저 로드\n","model_name = \"microsoft/graphcodebert-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.truncation_side = 'left'\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n","\n","\n","# 훈련 세트와 검증 세트에 대한 데이터셋 생성\n","train1_dataset = CodePairDataset(tokenizer, train1_df, max_length=512)\n","val_dataset = CodePairDataset(tokenizer, val_df, max_length=512, include_labels=True)\n","\n","# 데이터 로더 준비\n","train1_loader = DataLoader(train1_dataset, batch_size=48, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=48, shuffle=False)\n","\n","# 파인 튜닝을 위한 옵티마이저 설정\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","# 훈련 루프 수정\n","model.train()\n","for epoch in range(4):  # 에폭 수 필요에 따라 조정\n","    total_loss = 0\n","    model.train()\n","    for batch in tqdm(train1_loader):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # 에폭당 평균 훈련 손실 계산\n","    epoch_loss = total_loss / len(train1_loader)\n","    print(f\"Epoch {epoch+1}, Loss: {epoch_loss}\")\n","\n","    # 검증 세트를 이용한 모델 평가\n","    model.eval()\n","    total_eval_accuracy = 0\n","    for batch in tqdm(val_loader):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","\n","        logits = outputs.logits\n","        predictions = torch.argmax(logits, dim=-1)\n","        labels = batch[\"labels\"]\n","\n","        # 정확도 계산\n","        accuracy = (predictions == labels).cpu().numpy().mean() * 100\n","        total_eval_accuracy += accuracy\n","\n","    # 에폭당 평균 검증 정확도 계산\n","    avg_val_accuracy = total_eval_accuracy / len(val_loader)\n","    print(f\"Validation Accuracy: {avg_val_accuracy:.2f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hEv_cx2geAxw"},"outputs":[],"source":["# 첫번째 모델 저장\n","torch.save(model.state_dict(), './data/model_graphcodebert4.pth')"]},{"cell_type":"markdown","metadata":{"id":"tfvYvQ8WmcUy"},"source":["# 두번째 모델\n","\n","202만쌍 train, epoch 3, lr =2e-5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Nxdcq6Omblz"},"outputs":[],"source":["# GraphCodeBERT 모델 및 토크나이저 로드\n","# 초기화 해줌으로써, 첫번째 모델과 독립적으로 작동이 가능함\n","model_name = \"microsoft/graphcodebert-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.truncation_side = 'left'\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n","\n","\n","# 훈련 세트와 검증 세트에 대한 데이터셋 생성\n","train2_dataset = CodePairDataset(tokenizer, train2_df, max_length=512)\n","val_dataset = CodePairDataset(tokenizer, val_df, max_length=512, include_labels=True)\n","\n","# 데이터 로더 준비\n","train2_loader = DataLoader(train2_dataset, batch_size=48, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=48, shuffle=False)\n","\n","# 파인 튜닝을 위한 옵티마이저 설정\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","# 훈련 루프 수정\n","model.train()\n","for epoch in range(3):  # 에폭 수 필요에 따라 조정\n","    total_loss = 0\n","    model.train()\n","    for batch in tqdm(train2_loader):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # 에폭당 평균 훈련 손실 계산\n","    epoch_loss = total_loss / len(train2_loader)\n","    print(f\"Epoch {epoch+1}, Loss: {epoch_loss}\")\n","\n","    # 검증 세트를 이용한 모델 평가\n","    model.eval()\n","    total_eval_accuracy = 0\n","    for batch in tqdm(val_loader):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","\n","        logits = outputs.logits\n","        predictions = torch.argmax(logits, dim=-1)\n","        labels = batch[\"labels\"]\n","\n","        # 정확도 계산\n","        accuracy = (predictions == labels).cpu().numpy().mean() * 100\n","        total_eval_accuracy += accuracy\n","\n","    # 에폭당 평균 검증 정확도 계산\n","    avg_val_accuracy = total_eval_accuracy / len(val_loader)\n","    print(f\"Validation Accuracy: {avg_val_accuracy:.2f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iS69Rngpmpnj"},"outputs":[],"source":["# 두번째 모델 저장\n","torch.save(model.state_dict(), './data/model_graphcodebert6.pth')"]},{"cell_type":"markdown","metadata":{"id":"HtHI5P2emriq"},"source":["# 최종 모델\n","\n","저장 했던 두 모델을 불러와서 test데이터에 대한 예측을 수행합니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yn7h7W4imxnB"},"outputs":[],"source":["# 마지막으로 test데이터를 평가\n","test_data_path = './data/test.csv'\n","\n","# 테스트 데이터를 DataFrame으로 로드\n","test_df = pd.read_csv(test_data_path)\n","\n","# sample_submission.csv 파일 로딩\n","sample_submission_path = './data/sample_submission.csv'\n","sample_submission = pd.read_csv(sample_submission_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hhZORYSCnGyM"},"outputs":[],"source":["model_path_1 = './data/model_graphcodebert4.pth'\n","model_path_2 = './data/model_graphcodebert6.pth'\n","\n","# 모델 및 토크나이저 초기화\n","model_name = \"microsoft/graphcodebert-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.truncation_side = 'left'\n","\n","# 테스트 데이터셋 준비\n","test_dataset = CodePairDataset(tokenizer, test_df, max_length=512, include_labels=False)\n","test_loader = DataLoader(test_dataset, batch_size=48, shuffle=False)\n","\n","def evaluate_model(model_path):\n","    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","    model.eval()\n","\n","    probabilities = []\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            batch = {k: v.to(device) for k, v in batch.items()}\n","            outputs = model(**batch)\n","            logits = outputs.logits\n","            probs = torch.softmax(logits, dim=-1).cpu().numpy()  # 확률로 변환\n","            probabilities.extend(probs)\n","\n","    return np.array(probabilities)\n","\n","# 각 모델을 평가하여 확률을 얻음\n","probabilities_1 = evaluate_model(model_path_1)\n","probabilities_2 = evaluate_model(model_path_2)\n","\n","# 확률 평균을 최종 모델의 확률로 정함\n","final_probabilities = (probabilities_1 + probabilities_2) / 2\n","final_predictions = np.argmax(final_probabilities, axis=1)\n","\n","# 최종 예측 결과를 sample_submission.csv에 저장\n","sample_submission['similar'] = final_predictions\n","sample_submission.to_csv('./data/final_submission.csv', index=False)\n","\n","# 제출 파일 다운로드\n","#from google.colab import files\n","#files.download('./data/final_submission.csv')\n"]}],"metadata":{"colab":{"collapsed_sections":["o-9aGyX7m6j2","FIeWfNwHmz1N","yZpH35y3mHRb"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}